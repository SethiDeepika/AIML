{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119c20e7",
   "metadata": {
    "_cell_guid": "7ff76603-0955-461b-9190-7afac0bfabff",
    "_uuid": "9336de26-2683-4613-b96d-802f28dfa954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-06T03:12:37.220420Z",
     "iopub.status.busy": "2026-01-06T03:12:37.220176Z",
     "iopub.status.idle": "2026-01-06T03:29:14.525833Z",
     "shell.execute_reply": "2026-01-06T03:29:14.524909Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 997.310559,
     "end_time": "2026-01-06T03:29:14.527408",
     "exception": false,
     "start_time": "2026-01-06T03:12:37.216849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring 0.57 Champion (ResNet34) with Warm Restarts on cuda...\n",
      "\n",
      "=== FOLD 1/5 ===\n",
      "Fold 1 Best: 0.0294\n",
      "\n",
      "=== FOLD 2/5 ===\n",
      "Fold 2 Best: 0.0185\n",
      "\n",
      "=== FOLD 3/5 ===\n",
      "Fold 3 Best: 0.0192\n",
      "\n",
      "=== FOLD 4/5 ===\n",
      "Fold 4 Best: 0.0207\n",
      "\n",
      "=== FOLD 5/5 ===\n",
      "Fold 5 Best: 0.0237\n",
      "Starting 5-View TTA Inference...\n",
      "0.57+ Booster Submission Ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 320     # The winning resolution\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 2e-4\n",
    "N_FOLDS = 5\n",
    "IMAGE_DIR = Path(\"/kaggle/input/csiro-biomass\")\n",
    "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "print(f\"Restoring 0.57 Champion (ResNet34) with Warm Restarts on {DEVICE}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 4-CHANNEL DATASET (RGB + ExG)\n",
    "# ==========================================\n",
    "class Biomass4ChannelDataset(Dataset):\n",
    "    def __init__(self, df, target_cols=None, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_cols = target_cols\n",
    "        self.is_test = is_test\n",
    "        self.root_dir = IMAGE_DIR\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406, 0.5]).view(4,1,1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225, 0.5]).view(4,1,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.df.loc[idx, \"image_path\"]\n",
    "        img_path = self.root_dir / rel_path\n",
    "        try:\n",
    "            pil_img = Image.open(img_path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "            img = np.array(pil_img).astype(np.float32) / 255.0\n",
    "        except:\n",
    "            img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "\n",
    "        # ExG Index\n",
    "        r, g, b = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "        exg = (2 * g) - r - b\n",
    "        exg = (exg - exg.min()) / (exg.max() - exg.min() + 1e-6)\n",
    "        \n",
    "        img_4c = np.dstack((img, exg))\n",
    "        image = torch.tensor(img_4c.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            if np.random.random() > 0.5: image = torch.flip(image, dims=[2])\n",
    "            if np.random.random() > 0.5: image = torch.flip(image, dims=[1])\n",
    "\n",
    "        image = (image - self.mean) / self.std\n",
    "\n",
    "        if self.is_test:\n",
    "            img_id = Path(rel_path).stem \n",
    "            return image, img_id\n",
    "        else:\n",
    "            targets = self.df.loc[idx, self.target_cols].values.astype(float)\n",
    "            targets = np.log1p(targets) \n",
    "            return image, torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL (ResNet34)\n",
    "# ==========================================\n",
    "def get_model():\n",
    "    model = models.resnet34(weights=None)\n",
    "    \n",
    "    # Robust Weight Search\n",
    "    weights_path = None\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            if 'resnet34' in filename and '.pth' in filename:\n",
    "                weights_path = os.path.join(dirname, filename)\n",
    "                break\n",
    "        if weights_path: break\n",
    "            \n",
    "    if weights_path:\n",
    "        try: model.load_state_dict(torch.load(weights_path, weights_only=False))\n",
    "        except: pass\n",
    "    else:\n",
    "        # Fallback to 18\n",
    "        model = models.resnet18(weights=None)\n",
    "        for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "            for filename in filenames:\n",
    "                if 'resnet18' in filename and '.pth' in filename:\n",
    "                    weights_path = os.path.join(dirname, filename)\n",
    "                    break\n",
    "            if weights_path: break\n",
    "        if weights_path: model.load_state_dict(torch.load(weights_path, weights_only=False))\n",
    "\n",
    "    # Adapter\n",
    "    original_conv1 = model.conv1\n",
    "    model.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    with torch.no_grad():\n",
    "        model.conv1.weight[:, :3, :, :] = original_conv1.weight\n",
    "        model.conv1.weight[:, 3:4, :, :] = torch.mean(original_conv1.weight, dim=1, keepdim=True)\n",
    "\n",
    "    model.fc = nn.Linear(model.fc.in_features, 5)\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# ==========================================\n",
    "# 4. TRAINING ENGINE (MixUp + Warm Restarts)\n",
    "# ==========================================\n",
    "class WeightedHuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.huber = nn.HuberLoss(reduction='none', delta=delta)\n",
    "        self.weights = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5]).to(DEVICE)\n",
    "    def forward(self, preds, targets):\n",
    "        return (self.huber(preds, targets) * self.weights).mean()\n",
    "\n",
    "raw_df = pd.read_csv(\"/kaggle/input/csiro-biomass/train.csv\")\n",
    "train_pivot = raw_df.pivot(index='image_path', columns='target_name', values='target').reset_index().fillna(0.0)\n",
    "train_pivot['bin'] = pd.qcut(train_pivot['Dry_Total_g'], q=10, labels=False, duplicates='drop')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_pivot, train_pivot['bin'])):\n",
    "    print(f\"\\n=== FOLD {fold+1}/{N_FOLDS} ===\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        Biomass4ChannelDataset(train_pivot.iloc[train_idx], TARGET_COLS),\n",
    "        batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        Biomass4ChannelDataset(train_pivot.iloc[val_idx], TARGET_COLS),\n",
    "        batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    "    )\n",
    "    \n",
    "    model = get_model()\n",
    "    criterion = WeightedHuberLoss(delta=1.0)\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # --- UPGRADE: Warm Restarts Scheduler ---\n",
    "    # Resets LR every 5 epochs to find better minima\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # MixUp Logic (Winning Formula)\n",
    "            if np.random.random() < 0.5:\n",
    "                lam = np.random.beta(1.0, 1.0)\n",
    "                index = torch.randperm(x.size(0)).to(DEVICE)\n",
    "                mixed_x = lam * x + (1 - lam) * x[index]\n",
    "                \n",
    "                # Unlog -> Mix -> Relog\n",
    "                y_lin_a = torch.expm1(y)\n",
    "                y_lin_b = torch.expm1(y[index])\n",
    "                mixed_y = torch.log1p(lam * y_lin_a + (1 - lam) * y_lin_b)\n",
    "                \n",
    "                preds = model(mixed_x)\n",
    "                loss = criterion(preds, mixed_y)\n",
    "            else:\n",
    "                preds = model(x)\n",
    "                loss = criterion(preds, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Step scheduler per batch for Warm Restarts\n",
    "            scheduler.step(epoch + len(train_loader)/len(train_loader))\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in valid_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                val_loss += criterion(model(x), y).item()\n",
    "        \n",
    "        avg_val = val_loss / len(valid_loader)\n",
    "        \n",
    "        if avg_val < best_loss:\n",
    "            best_loss = avg_val\n",
    "            torch.save(model.state_dict(), f\"model_fold{fold}.pth\")\n",
    "            \n",
    "    print(f\"Fold {fold+1} Best: {best_loss:.4f}\")\n",
    "    \n",
    "    del model, optimizer, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ==========================================\n",
    "# 5. INFERENCE + 5-VIEW TTA\n",
    "# ==========================================\n",
    "print(\"Starting 5-View TTA Inference...\")\n",
    "test_df_raw = pd.read_csv(\"/kaggle/input/csiro-biomass/test.csv\")\n",
    "test_unique = test_df_raw[['image_path']].drop_duplicates()\n",
    "test_ds = Biomass4ChannelDataset(test_unique, is_test=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "ensemble_preds = {} \n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    model = get_model()\n",
    "    model.load_state_dict(torch.load(f\"model_fold{fold}.pth\", weights_only=True))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, img_ids in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            # 5-View TTA (Maximum Stability)\n",
    "            # 1. Normal\n",
    "            p1 = model(images)\n",
    "            # 2. Flip H\n",
    "            p2 = model(torch.flip(images, dims=[3]))\n",
    "            # 3. Flip V\n",
    "            p3 = model(torch.flip(images, dims=[2]))\n",
    "            # 4. Rot 90\n",
    "            p4 = model(torch.rot90(images, 1, [2, 3]))\n",
    "            # 5. Rot 270\n",
    "            p5 = model(torch.rot90(images, 3, [2, 3]))\n",
    "            \n",
    "            avg_log = (p1 + p2 + p3 + p4 + p5) / 5.0\n",
    "            preds = np.expm1(avg_log.cpu().numpy())\n",
    "            \n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                if img_id not in ensemble_preds: ensemble_preds[img_id] = np.zeros(5)\n",
    "                ensemble_preds[img_id] += preds[i]\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "results = []\n",
    "for img_id, total_preds in ensemble_preds.items():\n",
    "    avg_pred = total_preds / N_FOLDS\n",
    "    for j, col in enumerate(TARGET_COLS):\n",
    "        results.append({'sample_id': f\"{img_id}__{col}\", 'target': float(avg_pred[j])})\n",
    "\n",
    "submission_df = pd.DataFrame(results)\n",
    "submission_df['target'] = submission_df['target'].clip(lower=0.0)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"0.57+ Booster Submission Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9d9dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T03:29:14.532569Z",
     "iopub.status.busy": "2026-01-06T03:29:14.531952Z",
     "iopub.status.idle": "2026-01-06T03:29:14.549275Z",
     "shell.execute_reply": "2026-01-06T03:29:14.548605Z"
    },
    "papermill": {
     "duration": 0.02127,
     "end_time": "2026-01-06T03:29:14.550654",
     "exception": false,
     "start_time": "2026-01-06T03:29:14.529384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1001187975__Dry_Green_g</td>\n",
       "      <td>33.630044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1001187975__Dry_Dead_g</td>\n",
       "      <td>16.845873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1001187975__Dry_Clover_g</td>\n",
       "      <td>0.090285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1001187975__GDM_g</td>\n",
       "      <td>30.929482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001187975__Dry_Total_g</td>\n",
       "      <td>55.525881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id     target\n",
       "0   ID1001187975__Dry_Green_g  33.630044\n",
       "1    ID1001187975__Dry_Dead_g  16.845873\n",
       "2  ID1001187975__Dry_Clover_g   0.090285\n",
       "3         ID1001187975__GDM_g  30.929482\n",
       "4   ID1001187975__Dry_Total_g  55.525881"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 2847,
     "sourceId": 4958,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6885,
     "sourceId": 9959,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6978,
     "sourceId": 10038,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 23496,
     "sourceId": 30046,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 251095,
     "sourceId": 848739,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 504915,
     "sourceId": 936546,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 931479,
     "sourceId": 1575486,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1019605,
     "sourceId": 1718973,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1308452,
     "sourceId": 2179449,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1529762,
     "sourceId": 2524833,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3884593,
     "sourceId": 6746686,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3930781,
     "sourceId": 6836924,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1698210,
     "sourceId": 2781718,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 19591654,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1003.604654,
   "end_time": "2026-01-06T03:29:18.127416",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-06T03:12:34.522762",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
