{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93019a24",
   "metadata": {
    "_cell_guid": "7ff76603-0955-461b-9190-7afac0bfabff",
    "_uuid": "9336de26-2683-4613-b96d-802f28dfa954",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-06T04:18:04.579773Z",
     "iopub.status.busy": "2026-01-06T04:18:04.579539Z",
     "iopub.status.idle": "2026-01-06T04:41:35.212647Z",
     "shell.execute_reply": "2026-01-06T04:41:35.211677Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1410.638021,
     "end_time": "2026-01-06T04:41:35.214381",
     "exception": false,
     "start_time": "2026-01-06T04:18:04.576360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ResNet34 MixUp + SWA (Long Training) on cuda...\n",
      "\n",
      "=== FOLD 1/5 (MixUp + SWA) ===\n",
      "  Epoch 5 Val Loss: 0.0344\n",
      "  Epoch 10 Val Loss: 0.0325\n",
      "  Epoch 15 Val Loss: 0.0258\n",
      "  Epoch 20 Val Loss: 0.0270\n",
      "  Epoch 25 Val Loss: 0.0267\n",
      "Updating SWA Batch Norm...\n",
      "\n",
      "=== FOLD 2/5 (MixUp + SWA) ===\n",
      "  Epoch 5 Val Loss: 0.0275\n",
      "  Epoch 10 Val Loss: 0.0242\n",
      "  Epoch 15 Val Loss: 0.0165\n",
      "  Epoch 20 Val Loss: 0.0168\n",
      "  Epoch 25 Val Loss: 0.0184\n",
      "Updating SWA Batch Norm...\n",
      "\n",
      "=== FOLD 3/5 (MixUp + SWA) ===\n",
      "  Epoch 5 Val Loss: 0.0322\n",
      "  Epoch 10 Val Loss: 0.0243\n",
      "  Epoch 15 Val Loss: 0.0210\n",
      "  Epoch 20 Val Loss: 0.0205\n",
      "  Epoch 25 Val Loss: 0.0209\n",
      "Updating SWA Batch Norm...\n",
      "\n",
      "=== FOLD 4/5 (MixUp + SWA) ===\n",
      "  Epoch 5 Val Loss: 0.0322\n",
      "  Epoch 10 Val Loss: 0.0265\n",
      "  Epoch 15 Val Loss: 0.0191\n",
      "  Epoch 20 Val Loss: 0.0216\n",
      "  Epoch 25 Val Loss: 0.0209\n",
      "Updating SWA Batch Norm...\n",
      "\n",
      "=== FOLD 5/5 (MixUp + SWA) ===\n",
      "  Epoch 5 Val Loss: 0.0341\n",
      "  Epoch 10 Val Loss: 0.0291\n",
      "  Epoch 15 Val Loss: 0.0285\n",
      "  Epoch 20 Val Loss: 0.0247\n",
      "  Epoch 25 Val Loss: 0.0252\n",
      "Updating SWA Batch Norm...\n",
      "Starting Inference...\n",
      "SWA + MixUp + ResNet34 Submission Ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# SWA Imports\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SIZE = 320      # Winning Resolution\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 25         # INCREASED: MixUp needs time!\n",
    "SWA_START = 18      # Start averaging late in training\n",
    "LEARNING_RATE = 2e-4\n",
    "N_FOLDS = 5\n",
    "IMAGE_DIR = Path(\"/kaggle/input/csiro-biomass\")\n",
    "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "\n",
    "print(f\"Running ResNet34 MixUp + SWA (Long Training) on {DEVICE}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 4-CHANNEL DATASET (Winning Config)\n",
    "# ==========================================\n",
    "class Biomass4ChannelDataset(Dataset):\n",
    "    def __init__(self, df, target_cols=None, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_cols = target_cols\n",
    "        self.is_test = is_test\n",
    "        self.root_dir = IMAGE_DIR\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406, 0.5]).view(4,1,1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225, 0.5]).view(4,1,1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rel_path = self.df.loc[idx, \"image_path\"]\n",
    "        img_path = self.root_dir / rel_path\n",
    "        try:\n",
    "            pil_img = Image.open(img_path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "            img = np.array(pil_img).astype(np.float32) / 255.0\n",
    "        except:\n",
    "            img = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "\n",
    "        # Standard ExG\n",
    "        r, g, b = img[:,:,0], img[:,:,1], img[:,:,2]\n",
    "        exg = (2 * g) - r - b\n",
    "        exg = (exg - exg.min()) / (exg.max() - exg.min() + 1e-6)\n",
    "        \n",
    "        img_4c = np.dstack((img, exg))\n",
    "        image = torch.tensor(img_4c.transpose(2, 0, 1), dtype=torch.float32)\n",
    "        \n",
    "        if not self.is_test:\n",
    "            if np.random.random() > 0.5: image = torch.flip(image, dims=[2])\n",
    "            if np.random.random() > 0.5: image = torch.flip(image, dims=[1])\n",
    "\n",
    "        image = (image - self.mean) / self.std\n",
    "\n",
    "        if self.is_test:\n",
    "            img_id = Path(rel_path).stem \n",
    "            return image, img_id\n",
    "        else:\n",
    "            targets = self.df.loc[idx, self.target_cols].values.astype(float)\n",
    "            targets = np.log1p(targets)\n",
    "            return image, torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL (ResNet34)\n",
    "# ==========================================\n",
    "def get_model():\n",
    "    model = models.resnet34(weights=None)\n",
    "    # Search Weights\n",
    "    weights_path = None\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            if 'resnet34' in filename and '.pth' in filename:\n",
    "                weights_path = os.path.join(dirname, filename)\n",
    "                break\n",
    "        if weights_path: break\n",
    "            \n",
    "    if weights_path:\n",
    "        try: model.load_state_dict(torch.load(weights_path, weights_only=False))\n",
    "        except: pass\n",
    "    else:\n",
    "        # Fallback\n",
    "        model = models.resnet18(weights=None)\n",
    "        for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "            for filename in filenames:\n",
    "                if 'resnet18' in filename and '.pth' in filename:\n",
    "                    weights_path = os.path.join(dirname, filename)\n",
    "                    break\n",
    "            if weights_path: break\n",
    "        if weights_path: model.load_state_dict(torch.load(weights_path, weights_only=False))\n",
    "\n",
    "    # Adapter\n",
    "    original_conv1 = model.conv1\n",
    "    model.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    with torch.no_grad():\n",
    "        model.conv1.weight[:, :3, :, :] = original_conv1.weight\n",
    "        model.conv1.weight[:, 3:4, :, :] = torch.mean(original_conv1.weight, dim=1, keepdim=True)\n",
    "\n",
    "    # Deeper Head (Small Upgrade for capacity)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(model.fc.in_features, 5)\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# ==========================================\n",
    "# 4. WEIGHTED LOSS\n",
    "# ==========================================\n",
    "class WeightedHuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.huber = nn.HuberLoss(reduction='none', delta=delta)\n",
    "        self.weights = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5]).to(DEVICE)\n",
    "    def forward(self, preds, targets):\n",
    "        return (self.huber(preds, targets) * self.weights).mean()\n",
    "\n",
    "# ==========================================\n",
    "# 5. SWA TRAINING LOOP\n",
    "# ==========================================\n",
    "raw_df = pd.read_csv(\"/kaggle/input/csiro-biomass/train.csv\")\n",
    "train_pivot = raw_df.pivot(index='image_path', columns='target_name', values='target').reset_index().fillna(0.0)\n",
    "train_pivot['bin'] = pd.qcut(train_pivot['Dry_Total_g'], q=10, labels=False, duplicates='drop')\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_pivot, train_pivot['bin'])):\n",
    "    print(f\"\\n=== FOLD {fold+1}/{N_FOLDS} (MixUp + SWA) ===\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        Biomass4ChannelDataset(train_pivot.iloc[train_idx], TARGET_COLS),\n",
    "        batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        Biomass4ChannelDataset(train_pivot.iloc[val_idx], TARGET_COLS),\n",
    "        batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    "    )\n",
    "    \n",
    "    model = get_model()\n",
    "    swa_model = AveragedModel(model)\n",
    "    \n",
    "    criterion = WeightedHuberLoss(delta=1.0)\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Scheduler: Cosine until SWA start, then constant SWA LR\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SWA_START)\n",
    "    swa_scheduler = SWALR(optimizer, swa_lr=1e-5)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # --- MIXUP LOGIC ---\n",
    "            if np.random.random() < 0.5:\n",
    "                lam = np.random.beta(1.0, 1.0)\n",
    "                index = torch.randperm(x.size(0)).to(DEVICE)\n",
    "                mixed_x = lam * x + (1 - lam) * x[index]\n",
    "                # Unlog -> Mix -> Relog\n",
    "                y_lin_a = torch.expm1(y)\n",
    "                y_lin_b = torch.expm1(y[index])\n",
    "                mixed_y = torch.log1p(lam * y_lin_a + (1 - lam) * y_lin_b)\n",
    "                preds = model(mixed_x)\n",
    "                loss = criterion(preds, mixed_y)\n",
    "            else:\n",
    "                preds = model(x)\n",
    "                loss = criterion(preds, y)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # SWA Phase\n",
    "        if epoch >= SWA_START:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            \n",
    "        # Quick Val Check\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_loader:\n",
    "                    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                    val_loss += criterion(model(x), y).item()\n",
    "            print(f\"  Epoch {epoch+1} Val Loss: {val_loss/len(valid_loader):.4f}\")\n",
    "\n",
    "    # Finalize SWA\n",
    "    print(\"Updating SWA Batch Norm...\")\n",
    "    torch.optim.swa_utils.update_bn(train_loader, swa_model, device=DEVICE)\n",
    "    torch.save(swa_model.state_dict(), f\"model_fold{fold}.pth\")\n",
    "    \n",
    "    del model, swa_model, optimizer, train_loader, valid_loader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ==========================================\n",
    "# 6. INFERENCE\n",
    "# ==========================================\n",
    "print(\"Starting Inference...\")\n",
    "test_df_raw = pd.read_csv(\"/kaggle/input/csiro-biomass/test.csv\")\n",
    "test_unique = test_df_raw[['image_path']].drop_duplicates()\n",
    "test_ds = Biomass4ChannelDataset(test_unique, is_test=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "ensemble_preds = {} \n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    # Load SWA Model\n",
    "    base_model = get_model()\n",
    "    model = AveragedModel(base_model)\n",
    "    model.load_state_dict(torch.load(f\"model_fold{fold}.pth\", weights_only=True))\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, img_ids in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            # TTA: 3 Views\n",
    "            p1 = model(images)\n",
    "            p2 = model(torch.flip(images, dims=[3]))\n",
    "            p3 = model(torch.flip(images, dims=[2]))\n",
    "            \n",
    "            avg_log = (p1 + p2 + p3) / 3.0\n",
    "            preds = np.expm1(avg_log.cpu().numpy())\n",
    "            \n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                if img_id not in ensemble_preds: ensemble_preds[img_id] = np.zeros(5)\n",
    "                ensemble_preds[img_id] += preds[i]\n",
    "    \n",
    "    del model, base_model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "results = []\n",
    "for img_id, total_preds in ensemble_preds.items():\n",
    "    avg_pred = total_preds / N_FOLDS\n",
    "    for j, col in enumerate(TARGET_COLS):\n",
    "        results.append({'sample_id': f\"{img_id}__{col}\", 'target': float(avg_pred[j])})\n",
    "\n",
    "submission_df = pd.DataFrame(results)\n",
    "submission_df['target'] = submission_df['target'].clip(lower=0.0)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"SWA + MixUp + ResNet34 Submission Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a43bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T04:41:35.221103Z",
     "iopub.status.busy": "2026-01-06T04:41:35.220495Z",
     "iopub.status.idle": "2026-01-06T04:41:35.241150Z",
     "shell.execute_reply": "2026-01-06T04:41:35.240369Z"
    },
    "papermill": {
     "duration": 0.025551,
     "end_time": "2026-01-06T04:41:35.242604",
     "exception": false,
     "start_time": "2026-01-06T04:41:35.217053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID1001187975__Dry_Green_g</td>\n",
       "      <td>24.814009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID1001187975__Dry_Dead_g</td>\n",
       "      <td>28.725051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID1001187975__Dry_Clover_g</td>\n",
       "      <td>0.354886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID1001187975__GDM_g</td>\n",
       "      <td>22.247827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID1001187975__Dry_Total_g</td>\n",
       "      <td>50.685926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    sample_id     target\n",
       "0   ID1001187975__Dry_Green_g  24.814009\n",
       "1    ID1001187975__Dry_Dead_g  28.725051\n",
       "2  ID1001187975__Dry_Clover_g   0.354886\n",
       "3         ID1001187975__GDM_g  22.247827\n",
       "4   ID1001187975__Dry_Total_g  50.685926"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 2847,
     "sourceId": 4958,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6885,
     "sourceId": 9959,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6978,
     "sourceId": 10038,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 23496,
     "sourceId": 30046,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 251095,
     "sourceId": 848739,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 504915,
     "sourceId": 936546,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 931479,
     "sourceId": 1575486,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1019605,
     "sourceId": 1718973,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1308452,
     "sourceId": 2179449,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1529762,
     "sourceId": 2524833,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3884593,
     "sourceId": 6746686,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3930781,
     "sourceId": 6836924,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1698210,
     "sourceId": 2781718,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 19591654,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1417.430139,
   "end_time": "2026-01-06T04:41:38.341046",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-06T04:18:00.910907",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
